Keyword fields:
Om filteren op land mogelijk te maken moet dit veld 'keyword' zijn, niet text.
Getest via 're-index', maar de index moet automatisch goed gemaakt worden.
	- Opschonen code 
	- UI opschonen
	- Paginering stukje (wat niet werkt nog) weghalen
	- Nice to have: Ajax call voor search result

Australia:
	- .xlsx > .csv script testen
	- Makefile aanpassen zodat .xlsx naar json gaat
		- Ik probeer eerst optie D
			- A. binnen #Download een if statement
				- dan conversie script daar ook binnen
				- zodat dan uiteindelijk ook een raw.csv ontstaat
				- Maar is lelijk dat het 2 dingen doet
			- B. Download via een python script
				- Die gebruikt standaard curl
				- Maar kan per land eventueel extra stappen doen
				- Maar alsnog lelijk dat het 2 dingen doet
				- En als je dan toch python script hebt..
					- Dan kun je net zo goed meteen door naar .json conversie
			- C. Elke data folder heeft eigen Makefile
				- Root makefile roept make van folder aan
				- De data folder make roept bijna altijd generieke make file aan
				- Maar voor 'alternatieve' bouw dan uniek makefile.
				- Lelijk is hier complexiteit
			- D. Downloaden .xlsx onterecht als raw.csv
				- bij rawutf8.csv doen we niets (copy)
				- bij cleanCSV.py doen we conversie van xlsx naar json
				- Elegant is dat makefile gelijk blijft
				- Lelijk is dat er .csv bestanden komen die niet csv zijn
					- Maar kan toegelicht worden in source.md
					- Kan ook comment bij Makefile gemaakt worden
					- En bronbestanden zijn tussenproduct voor debugging niet eindgebruiker

				
		- Als sourcefile .xlsx bestand is, dan ook convert script aanroepen
	- .xlsx > .csv script toevoegen
	- cleanCSV.py voor Australia maken
	- Requirements.txt voor deployment het xlsx2csv script toevoegen
	- Lokaal testen totaal
	- Live deployment

filters:
	- Vermoedelijk is 'Country' verkeerde field type. Textfield,
	  maar moet een 'keyword' datatype zijn oid.
		- https://discuss.elastic.co/t/how-to-change-the-field-type-of-data-loaded-in-kibana/136909/1
		- Mapping van field types, moet bij begin,
		- Dus kan nu alleen nieuwe mapping maken voor nieuwe index en dan overhevelen.
		- heirvoor is reindex gemaakt
			- Dus ik kan nieuwe mapping maken en dan via re-index het overzetten
			- Dit is goed voor testen, of dan query wel werkt.
			- Mocht het werken, dan kan ik daarna uitzoeken..
			- .. hoe ik vanaf niets direct goed kan uploaden
				- Dus of eerst apart mapping neerzetten, dan uploaden
				- ..Of bij uploaden veld type ook beschrijven
				  Maar lijkt mij dat bronbestand dan onnodig groot wordt.
https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-multi-match-query.html

Pagineren:
	- Nog niet functioneel
		- uitlezen uit url 'page' variabele
		- Bij template meegeven welke 'page' we op zijn
		- Url maken met huidige page 'show more' link.
	- Functioneel
		- 'page' gebruiken om juiste resultaten te tonen

Safety:
	- Research risico van dergelijke oplossing, kan je nu ES leegmaken met juiste HTML call?
		- Want ik pak waarde checkbox en voer het indirect aan Elasticsearch..

Digital ocean setup:
	- alle stappen in README.md zetten
	- apt-get install tree
	- apt-get install python3-pip (ofzoiets)
	- apt-get install make (ofzoiets)
	- symlink override docker-compose.override.yml maken (?)
		- We hebben in ieder geval geen Kibana nodig (scheelt ook ram)
	- omdat te weinig ram is voor elasticsearch, swap file bepalen
	   sudo fallocate -l 1G /swapfile
	- eable swap file
	  https://www.digitalocean.com/community/tutorials/how-to-add-swap-space-on-ubuntu-18-04
      sudo chmod 600 /swapfile

	  Verify the permissions change by typing
      ls -lh /swapfile	
	- Poging met 2Gb droplet en volgende commando was succes
      sysctl -w vm.max_map_count=262144
      stond hier: https://github.com/maxyermayank/docker-compose-elasticsearch-kibana/issues/10

Volume mapping voor ES:
	- content van db in volume houden voor geval container down gaat

Zoek verbeteren:
Mogelijke feature van zoekmachine is dat je veel kunt customizen
Dat je bijv. zelf fuzziness kunt selecteren, etc.
	- Land filteren
	- Fuzzy
	- Misschien tokenizen
	- Kibana voor monitoring kan ook nuttig zijn
	- AJAX

Log:
	- Moet ook wegschrijven in een .log bestand 
     Dat bijhoudt waneer welk bestand gemaakt is e.d.

USA4:
	- usa4 zit in andere landen, dat clean bestand moet anders

Betere scripts:
	- Opschonen scripts (alles met python argv paden)
	- Bewaar niet meer tussenstappen, maar wel het raw bestand?
	Liefst variant met copy met datum stamp
	Dus dat die backup wel bewaard blijft, maar niet bronbestand
	- SourceURL en SourceDate in een soort config bestand ergens zetten
	of hard in make bestand zetten die de functies aanroept,
	zodat makefile set variabelen bovenin heeft die effectief config is.
	- SourceURL
	- DateOfSource
	- Kan make/bash bestand kijken of ES al bestanden heeft van dit land?
		- Zonee, upload deze dan
		- Zoja, verwijder dan deze voor dit land eerst, en voeg ze daarna toe

map structuur:
- data/<land>/
	- <land>.raw.csv     zoals gedownload
	- <land>.clean.csv   klaar voor toJson
- scripts/<land>/
	- cleanCSV.py        maakt van .nonull.csv het .clean.csv bestand

Bronnen:

https://www.youtube.com/watch?v=b7tCjZSvOno
Video met in beeld voorbeelden van gebruik tokenizer e.d. 

https://dev.to/aligoren/using-elasticsearch-with-python-and-flask-2i0e

https://blog.patricktriest.com/text-search-docker-elasticsearch/

https://github.com/triestpa/guttenberg-search

Requirements.txt & conda:
https://medium.com/@boscacci/why-and-how-to-make-a-requirements-txt-f329c685181e

Doel:
Charitius 2.0
Een open data set van goede doelen pakken en deze zoekbaar maken.
Eerst alleen op titel, dan ook 'full-text' indexing.

Als het ok werkt, dan ook op Digital Ocean deployen. 
Als dat ok werkt, dan steeds meer open data toevoegen.
Geen webscraping, maar alleen van officiele bronnen.
