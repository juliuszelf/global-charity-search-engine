Scotland:
	- Komt binnen als .zip die een .csv bevat
	- Voeg aan README.md de requirement 'unzip' installeren toe.
	- Wat is beste wijze?
		- Ik ga voor B
		- A. download_unzip.sh script maken
			- Roep je aan vanuit make
			- Zit in map van land
			- Doet niet altijd een unzip, alleen als ergens op slaat
			- Lelijk is 2 stappen in 1 script
			- Voordeel is dat rest van makefile en scripts gelijk blijft
		- B. Aparte Makefile stap
			- download in generiek 'downloaded.file' 
			- dan een script voor 'file_to_raw_csv.py' oid.
				- Doet sowieso ook utf8 fix
			- Bij .zip dus een unzip ook
		- C. Houden Makefile zoals het is
			- De raw.csv is dan eigenlijk dus een zip
			- Bij cleanCSV.py gebeurt het eigenlijk unzippen e.d.

Backup:
	- Een backup stap in makefile
	- Dus die downloaded.file pakt en in (gitignored) backup map plaatst
	- format: <folder>_<date>.backup
	- In zelfde map ook een log > welk bestand weggeschreven en wat bron url was.
	- Checked eerst of gedownloade bestand andere grootte heeft dan laatste backup
		- Bij gelijke grootte aanname zelfde bestand, dus geen extra backup nodig.
	- In README toelichten hoe je backup kan terugzetten.
		- Eventueel script maken die dit voor je doet.

Categorien:
	- Invetariseren
		- wat levert bronbestanden?
		- Technische uitdagingen?

Land van opereren:
	- Invetariseren
		- wat levert bronbestanden?
		- Technische uitdagingen?

Mogelijke landen:
	- Met source.md
		- UK  
		- Scotland
		- North-Ireland
	- Norway
		- https://data.ssb.no/api/v0/dataset/?lang=en
	- USA4 
		- zit in andere landen, dat clean bestand moet anders
	- Rusland
		- https://data.gov.ru/taxonomy/term/71/datasets
			- Niet duidelijk of daar iets nuttig is

Pagineren:
	- Nog niet functioneel
		- uitlezen uit url 'page' variabele
		- Bij template meegeven welke 'page' we op zijn
		- Url maken met huidige page 'show more' link.
	- Functioneel
		- 'page' gebruiken om juiste resultaten te tonen
	- Nice to have: Ajax call voor search result

Safety:
	- Research risico van dergelijke oplossing, kan je nu ES leegmaken met juiste HTML call?
		- Want ik pak waarde checkbox en voer het indirect aan Elasticsearch..

Digital ocean setup:
	- alle stappen in README.md zetten
	- Dit zou in een script kunnen:
		- setup-server.sh
			- Comment dat Digital Ocean afgestemd is
			- apt-get install tree
			- apt-get install python3-pip (ofzoiets)
			- apt-get install make (ofzoiets)
			- symlink override docker-compose.override.yml maken (?)
				- We hebben in ieder geval geen Kibana nodig (scheelt ook ram)
			- omdat te weinig ram is voor elasticsearch, swap file bepalen
			sudo fallocate -l 1G /swapfile
			- eable swap file
			https://www.digitalocean.com/community/tutorials/how-to-add-swap-space-on-ubuntu-18-04
			sudo chmod 600 /swapfile

			Verify the permissions change by typing
			ls -lh /swapfile	
			- Poging met 2Gb droplet en volgende commando was succes
			sysctl -w vm.max_map_count=262144
			stond hier: https://github.com/maxyermayank/docker-compose-elasticsearch-kibana/issues/10

Zoek verbeteren:
Mogelijke feature van zoekmachine is dat je veel kunt customizen
Dat je bijv. zelf fuzziness kunt selecteren, etc.
	- Land filteren @done(2019-12-06)
	- Toon bronbestand & bron datum
	- Fuzzy
	- AJAX
	- Analytics
	- Kibana voor monitoring kan ook nuttig zijn

Log:
Moet ook wegschrijven in een .log bestand 
	- Dat bijhoudt waneer welk bestand gemaakt is e.d.
	- Schrijf templog van elke stap
	- Als alles succesvol delete temp log & schrijf 'make success' met datum
	- Bij falen zet hele temp log in het echte log

Scripts verbeteren:
	- SourceURL en SourceDate in een soort config bestand ergens zetten
	of hard in make bestand zetten die de functies aanroept,
	zodat makefile set variabelen bovenin heeft die effectief config is.
	- Een 'update script' die checkt wat er al is, etc.
	- Kan make/bash bestand kijken of ES al bestanden heeft van dit land?
		- Zonee, upload deze dan
		- Zoja, verwijder dan deze voor dit land eerst, en voeg ze daarna toe

map structuur:
- data/<land>/
	- <land>.raw.csv     zoals gedownload
	- <land>.clean.csv   klaar voor toJson
- scripts/<land>/
	- cleanCSV.py        maakt van .nonull.csv het .clean.csv bestand

Bronnen:

https://www.youtube.com/watch?v=b7tCjZSvOno
Video met in beeld voorbeelden van gebruik tokenizer e.d. 

https://dev.to/aligoren/using-elasticsearch-with-python-and-flask-2i0e

https://blog.patricktriest.com/text-search-docker-elasticsearch/

https://github.com/triestpa/guttenberg-search

Requirements.txt & conda:
https://medium.com/@boscacci/why-and-how-to-make-a-requirements-txt-f329c685181e

Doel:
Charitius 2.0
Een open data set van goede doelen pakken en deze zoekbaar maken.
Eerst alleen op titel, dan ook 'full-text' indexing.

Als het ok werkt, dan ook op Digital Ocean deployen. 
Als dat ok werkt, dan steeds meer open data toevoegen.
Geen webscraping, maar alleen van officiele bronnen.
