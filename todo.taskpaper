Keyword fields:
Om filteren op land mogelijk te maken moet dit veld 'keyword' zijn, niet text.
Getest via 're-index', maar de index moet automatisch goed gemaakt worden.
	- Experiment: Kan ik ook uploaden met alleen op 1e regel _index zetten.
	- Nette variant lijkt 3 (mapping als stap 1)
		- Geeft direct duidelijke plek van configuratie (=documentatie)
		- Variant 1
		Dit heb ik met de hand gedaan en werkt
			- Eerst via uploaden maken
			- Dan juiste mapping
			- Dan re-index
		- Variant 2
			- Eerst index aanmaken met juiste mapping
			- Dan upload bronbestanden maken
		- Variant 3
			- Via upload index aanmaken, maar json zo maken dat veld 'keyword' ook goed gaat.
	- Dus moet het upload-all.sh script aangepast worden door eerst mapping te uploaden
		- create-index-with-mapping.sh @done(2019-12-03)
		- Vermoedelijk kunnen bronbestanden dan gelijk blijven.

filters:
	- Vermoedelijk is 'Country' verkeerde field type. Textfield,
	  maar moet een 'keyword' datatype zijn oid.
		- https://discuss.elastic.co/t/how-to-change-the-field-type-of-data-loaded-in-kibana/136909/1
		- Mapping van field types, moet bij begin,
		- Dus kan nu alleen nieuwe mapping maken voor nieuwe index en dan overhevelen.
		- heirvoor is reindex gemaakt
			- Dus ik kan nieuwe mapping maken en dan via re-index het overzetten
			- Dit is goed voor testen, of dan query wel werkt.
			- Mocht het werken, dan kan ik daarna uitzoeken..
			- .. hoe ik vanaf niets direct goed kan uploaden
				- Dus of eerst apart mapping neerzetten, dan uploaden
				- ..Of bij uploaden veld type ook beschrijven
				  Maar lijkt mij dat bronbestand dan onnodig groot wordt.
https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-multi-match-query.html

Pagineren:
	- Nog niet functioneel
		- uitlezen uit url 'page' variabele
		- Bij template meegeven welke 'page' we op zijn
		- Url maken met huidige page 'show more' link.
	- Functioneel
		- 'page' gebruiken om juiste resultaten te tonen

Digital ocean setup:
	- alle stappen in README.md zetten
	- apt-get install tree
	- apt-get install python3-pip (ofzoiets)
	- apt-get install make (ofzoiets)
	- symlink override docker-compose.override.yml maken (?)
		- We hebben in ieder geval geen Kibana nodig (scheelt ook ram)
	- omdat te weinig ram is voor elasticsearch, swap file bepalen
	   sudo fallocate -l 1G /swapfile
	- eable swap file
	  https://www.digitalocean.com/community/tutorials/how-to-add-swap-space-on-ubuntu-18-04
      sudo chmod 600 /swapfile

	  Verify the permissions change by typing
      ls -lh /swapfile	
	- Poging met 2Gb droplet en volgende commando was succes
      sysctl -w vm.max_map_count=262144
      stond hier: https://github.com/maxyermayank/docker-compose-elasticsearch-kibana/issues/10

Volume mapping voor ES:
	- content van db in volume houden voor geval container down gaat

Zoek verbeteren:
Mogelijke feature van zoekmachine is dat je veel kunt customizen
Dat je bijv. zelf fuzziness kunt selecteren, etc.
	- Land filteren
	- Fuzzy
	- Misschien tokenizen
	- Kibana voor monitoring kan ook nuttig zijn
	- AJAX

Log:
	- Moet ook wegschrijven in een .log bestand 
     Dat bijhoudt waneer welk bestand gemaakt is e.d.

USA4:
	- usa4 zit in andere landen, dat clean bestand moet anders

Betere scripts:
	- Opschonen scripts (alles met python argv paden)
	- Bewaar niet meer tussenstappen, maar wel het raw bestand?
	Liefst variant met copy met datum stamp
	Dus dat die backup wel bewaard blijft, maar niet bronbestand
	- SourceURL en SourceDate in een soort config bestand ergens zetten
	of hard in make bestand zetten die de functies aanroept,
	zodat makefile set variabelen bovenin heeft die effectief config is.
	- SourceURL
	- DateOfSource
	- Kan make/bash bestand kijken of ES al bestanden heeft van dit land?
		- Zonee, upload deze dan
		- Zoja, verwijder dan deze voor dit land eerst, en voeg ze daarna toe

map structuur:
- data/<land>/
	- <land>.raw.csv     zoals gedownload
	- <land>.clean.csv   klaar voor toJson
- scripts/<land>/
	- cleanCSV.py        maakt van .nonull.csv het .clean.csv bestand

Bronnen:

https://www.youtube.com/watch?v=b7tCjZSvOno
Video met in beeld voorbeelden van gebruik tokenizer e.d. 

https://dev.to/aligoren/using-elasticsearch-with-python-and-flask-2i0e

https://blog.patricktriest.com/text-search-docker-elasticsearch/

https://github.com/triestpa/guttenberg-search

Requirements.txt & conda:
https://medium.com/@boscacci/why-and-how-to-make-a-requirements-txt-f329c685181e

Doel:
Charitius 2.0
Een open data set van goede doelen pakken en deze zoekbaar maken.
Eerst alleen op titel, dan ook 'full-text' indexing.

Als het ok werkt, dan ook op Digital Ocean deployen. 
Als dat ok werkt, dan steeds meer open data toevoegen.
Geen webscraping, maar alleen van officiele bronnen.
