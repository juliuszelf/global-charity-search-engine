Doel:
Charitius 2.0
Een open data set van goede doelen pakken en deze zoekbaar maken.
Eerst alleen op titel, dan ook 'full-text' indexing.

Als het ok werkt, dan ook op Digital Ocean deployen. 
Als dat ok werkt, dan steeds meer open data toevoegen.
Geen webscraping, maar alleen van officiele bronnen.

Generiek makefile:
 - Opschonen scripts (alles met python argv paden)
 - Bewaar niet meer tussenstappen, maar wel het raw bestand?
   Liefst variant met copy met datum stamp
   Dus dat die backup wel bewaard blijft, maar niet bronbestand
   
 x toJson via externe parameters voor bestandspaden
   dan gaat pad verwijzing goed ongeacht waar je start.
  - Country namen als variabelen
  - Lijst gebruiken in 'chain' vorm
    - dus dat voor alle stappen die ik heb telkens die country-name ingevuld wordt


2e land toevoegen:
  - scripts zoals bij Canada
     - Waar mogelijk generiek opzetten, make bestand mappen af, etc
  - ES upload vooralsnog met de hand houden ivm afhankelijkheid docker.

Log:
   - Moet ook wegschrijven in een .log bestand 
     Dat bijhoudt waneer welk bestand gemaakt is e.d.

Automatiseren geheel:
 - SourceURL en SourceDate in een soort config bestand ergens zetten
   of hard in make bestand zetten die de functies aanroept,
   zodat makefile set variabelen bovenin heeft die effectief config is.
   - SourceURL
   - DateOfSource
 - Kan make/bash bestand kijken of ES al bestanden heeft van dit land?
     - Zonee, upload deze dan
     - Zoja, verwijder dan deze voor dit land eerst, en voeg ze daarna toe

Meer:
  x Docker voor Elastic search online
    - puur via kibana doe..
      - tokenizen
      - een zoek opdracht over meerdere data
  - In zelfde network ook Flask
  - Flask laten praten met Elastic
  - Data uitlezen met Flask met Flask
  - Kibana voor monitoring kan ook nuttig zijn

map structuur:
 - data/<land>/
    - <land>.raw.csv     zoals gedownload
    - <land>.clean.csv   klaar voor toJson
 - scripts/<land>/
    - cleanCSV.py        maakt van .nonull.csv het .clean.csv bestand

Bronnen:

https://www.youtube.com/watch?v=b7tCjZSvOno
Video met in beeld voorbeelden van gebruik tokenizer e.d. 

https://dev.to/aligoren/using-elasticsearch-with-python-and-flask-2i0e

https://blog.patricktriest.com/text-search-docker-elasticsearch/

https://github.com/triestpa/guttenberg-search

Requirements.txt & conda:
https://medium.com/@boscacci/why-and-how-to-make-a-requirements-txt-f329c685181e
