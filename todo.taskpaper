Scotland:
	- Komt binnen als .zip die een .csv bevat
	- Wat is beste wijze?
		- A. download_unzip.sh script maken
			- Roep je aan vanuit make
			- Zit in map van land
			- Doet niet altijd een unzip, alleen als ergens op slaat
			- Lelijk is 2 stappen in 1 script
			- Voordeel is dat rest van makefile en scripts gelijk blijft
		- B. Aparte Makefile stap
			- download in generiek 'download.file' 
			- dan een script voor 'file_to_csv.py' oid.
			- die laat je in meeste gevallen alleen het hernoeme van bestand doen
			- Lelijk is onnodig werk voor meeste gevallen
		- C. Houden Makefile zoals het is
			- De raw.csv is dan eigenlijk dus een zip
			- Bij cleanCSV.py gebeurt het eigenlijk unzippen e.d.
	- Herzien wat er gebeurt
		- Er is een download
		- Deze moet omgezet worden tot de correcte json
		- Dit kan in theorie allemaal in 1 python script gebeuren.

Mogelijke landen:
	- Met source.md
		- UK  
		- Scotland
		- North-Ireland
	- Norway
		- https://data.ssb.no/api/v0/dataset/?lang=en
	- Rusland
		- https://data.gov.ru/taxonomy/term/71/datasets
			- Niet duidelijk of daar iets nuttig is

Pagineren:
	- Nog niet functioneel
		- uitlezen uit url 'page' variabele
		- Bij template meegeven welke 'page' we op zijn
		- Url maken met huidige page 'show more' link.
	- Functioneel
		- 'page' gebruiken om juiste resultaten te tonen
	- Nice to have: Ajax call voor search result

Safety:
	- Research risico van dergelijke oplossing, kan je nu ES leegmaken met juiste HTML call?
		- Want ik pak waarde checkbox en voer het indirect aan Elasticsearch..

Digital ocean setup:
	- alle stappen in README.md zetten
	- apt-get install tree
	- apt-get install python3-pip (ofzoiets)
	- apt-get install make (ofzoiets)
	- symlink override docker-compose.override.yml maken (?)
		- We hebben in ieder geval geen Kibana nodig (scheelt ook ram)
	- omdat te weinig ram is voor elasticsearch, swap file bepalen
	   sudo fallocate -l 1G /swapfile
	- eable swap file
	  https://www.digitalocean.com/community/tutorials/how-to-add-swap-space-on-ubuntu-18-04
      sudo chmod 600 /swapfile

	  Verify the permissions change by typing
      ls -lh /swapfile	
	- Poging met 2Gb droplet en volgende commando was succes
      sysctl -w vm.max_map_count=262144
      stond hier: https://github.com/maxyermayank/docker-compose-elasticsearch-kibana/issues/10

Volume mapping voor ES:
	- content van db in volume houden voor geval container down gaat

Zoek verbeteren:
Mogelijke feature van zoekmachine is dat je veel kunt customizen
Dat je bijv. zelf fuzziness kunt selecteren, etc.
	- Land filteren
	- Fuzzy
	- Misschien tokenizen
	- Kibana voor monitoring kan ook nuttig zijn
	- AJAX

Log:
	- Moet ook wegschrijven in een .log bestand 
     Dat bijhoudt waneer welk bestand gemaakt is e.d.

USA4:
	- usa4 zit in andere landen, dat clean bestand moet anders

Betere scripts:
	- Opschonen scripts (alles met python argv paden)
	- Bewaar niet meer tussenstappen, maar wel het raw bestand?
	Liefst variant met copy met datum stamp
	Dus dat die backup wel bewaard blijft, maar niet bronbestand
	- SourceURL en SourceDate in een soort config bestand ergens zetten
	of hard in make bestand zetten die de functies aanroept,
	zodat makefile set variabelen bovenin heeft die effectief config is.
	- SourceURL
	- DateOfSource
	- Kan make/bash bestand kijken of ES al bestanden heeft van dit land?
		- Zonee, upload deze dan
		- Zoja, verwijder dan deze voor dit land eerst, en voeg ze daarna toe

map structuur:
- data/<land>/
	- <land>.raw.csv     zoals gedownload
	- <land>.clean.csv   klaar voor toJson
- scripts/<land>/
	- cleanCSV.py        maakt van .nonull.csv het .clean.csv bestand

Bronnen:

https://www.youtube.com/watch?v=b7tCjZSvOno
Video met in beeld voorbeelden van gebruik tokenizer e.d. 

https://dev.to/aligoren/using-elasticsearch-with-python-and-flask-2i0e

https://blog.patricktriest.com/text-search-docker-elasticsearch/

https://github.com/triestpa/guttenberg-search

Requirements.txt & conda:
https://medium.com/@boscacci/why-and-how-to-make-a-requirements-txt-f329c685181e

Doel:
Charitius 2.0
Een open data set van goede doelen pakken en deze zoekbaar maken.
Eerst alleen op titel, dan ook 'full-text' indexing.

Als het ok werkt, dan ook op Digital Ocean deployen. 
Als dat ok werkt, dan steeds meer open data toevoegen.
Geen webscraping, maar alleen van officiele bronnen.
